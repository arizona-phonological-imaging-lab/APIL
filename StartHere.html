<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Arizona Phonological Imaging Lab</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Apil</h1>
      <h2 class="project-tagline">APIL tools</h2>
      <a href="https://github.com/arizona-phonological-imaging-lab/Autotrace" class="btn">View on GitHub</a>
      <a href="https://github.com/arizona-phonological-imaging-lab/Autotrace/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/arizona-phonological-imaging-lab/Autotrace/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="apil" class="anchor" href="#apil" aria-hidden="true"><span class="octicon octicon-link"></span></a>APIL</h1>


<blockquote>
<p>This manual will explain the process of how to use Autotrace to trace images, train a network, and generate traces. </p>
</blockquote>
<h2 id="tracing-manually">Tracing Manually</h2>
<ul>
<li>Open <a href="autotrace.html#manual">Autotrace</a> and follow the manual instructions. <ul>
<li>If you do not already have traced data, it is recommended that you use this manual method to generate a <strong>training set</strong> upon which to construct the deep belief network as explained below. </li>
</ul>
</li>
</ul>
<h2 id="tracing-automatically">Tracing Automatically</h2>
<ul>
<li><p>Before anything, your images must be in .jpg format, <em>not</em> .png. If you have .png images, convert them using the included <strong>png_to_jpg.py</strong> script, by typing <code>python png_to_jpg.py /path/to/png/images</code>. </p>
</li>
<li><p>First, use <a href="selectroi.html">SelectROI</a> to create a ROIconfig.txt file. Remember where you put it, we may need to move it later. </p>
</li>
<li><p>Next, use <a href="imagediversityNEW.html">ImageDiversity</a> to separate a training and test set. </p>
</li>
<li><p>Then, use <a href="configdir.html">ConfigDir</a> to arrange the training data into the proper directory structure. </p>
<ul>
<li>At this point, ROIconfig.txt should be in the /train/Subject1/ directory. If it is not, move it there. </li>
</ul>
</li>
<li><p>Now for the actual training. Follow the instructions in <a href="TrainNetwork.html">Train Network</a>. </p>
<ul>
<li>Ensure you point TrainNetwork at train/Subject1/ <em>and</em> train/traces using shift-click.</li>
<li>This may take a while. Be patient. </li>
<li>Now you have a Deep-Belief Network tailored to your data set that will be used by Autotrace. </li>
</ul>
</li>
<li><p>Now you are ready to use Autotrace. Open <a href="autotrace.html">the manual</a> and scroll down to the automatic tracing instructions.  </p>
</li>
</ul>
<footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/arizona-phonological-imaging-lab/Autotrace">Apil</a> is maintained by <a href="https://github.com/Trevortds">Trevortds</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

